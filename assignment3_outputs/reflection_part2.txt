Reflection (Part 2 - Legal Case Similarity Retrieval)
-----------------------------------------------------
Process and Method:
Each caseâ€™s extractive summary was combined into a single text document.
We generated semantic embeddings for every document using the pretrained
SentenceTransformer model 'all-MiniLM-L6-v2', which converts text into
dense vectors representing meaning. Cosine similarity was then used to
measure how closely cases relate semantically.

Results and Interpretation:
The retrieval system outputs the Top-K (here, 5) most similar cases for
each query case, with similarity scores between 0 and 1. Higher scores
mean stronger semantic overlap in facts, legal arguments, or cited
statutes. For example, criminal petitions tend to retrieve other
criminal-law cases with similar procedural language. This shows that
embedding-based similarity captures meaningful relationships even when
wording differs.

Improvements and Insights:
This simple embedding approach already performs well for clustering
related judgments. However, a domain-specific legal model or inclusion
of metadata (e.g., court, statute sections) could further improve
accuracy for nuanced legal issues.